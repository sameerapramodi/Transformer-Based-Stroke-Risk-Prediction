# -*- coding: utf-8 -*-
"""LR,SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yIk1IrHpfanzroqJEN0fB5Gv3aNXc1qp
"""



# 1. Install required libraries (only run once)
# You can uncomment and run these if libraries are not installed
# !pip install pandas numpy scikit-learn matplotlib seaborn imbalanced-learn missingno plotly

# 2. Load required libraries

import pandas as pd                # For data manipulation (similar to dplyr in R)
import numpy as np                 # For numerical operations
import matplotlib.pyplot as plt    # For creating plots
import seaborn as sns              # For better visualizations (similar to ggplot2 in R)
import missingno as msno           # For visualizing missing data (similar to naniar and VIM)
from sklearn.model_selection import train_test_split  # For train-test splitting
from sklearn.preprocessing import StandardScaler      # For feature scaling (standardization)
from sklearn.svm import SVC                           # For Support Vector Machine model
from sklearn.linear_model import LogisticRegression   # For Logistic Regression
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, ConfusionMatrixDisplay  # Evaluation tools
from imblearn.combine import SMOTEENN                 # To balance dataset (similar to ROSE in R)

from google.colab import files
uploaded = files.upload()

import pandas as pd
import io

# Load the uploaded file
stroke_data = pd.read_csv('stroke_prediction_dataset.csv')

# Show the first few rows
stroke_data.head()

# 4. Explore dataset
print(stroke_data.info())
print(stroke_data.describe())

# 5. Visualize missing data
msno.bar(stroke_data)
plt.title("Missing Values per Column")
plt.show()

msno.matrix(stroke_data)
plt.title("Missing Data Matrix")
plt.show()

print("Missing Values After Handling:")
print(stroke_data.isnull().sum())

# 6. Remove rows with missing values (or use imputation if needed)
stroke_data.dropna(inplace=True)

# 4. Convert categorical columns to category dtype
categorical_cols = ["Gender", "Marital_Status", "Work_Type", "Residence_Type",
                    "Smoking_Status", "Alcohol_Intake", "Physical_Activity",
                    "Stroke_History", "Family_History_of_Stroke", "Dietary_Habits",
                    "Stress_Levels", "Blood_Pressure_Levels", "Cholesterol_Levels",
                    "Symptoms", "Diagnosis"]

# Clean column names to avoid issues with spaces
stroke_data.columns = stroke_data.columns.str.strip()

# Convert categorical columns
for col in categorical_cols:
    if col in stroke_data.columns:
        stroke_data[col] = stroke_data[col].astype("category")

print(stroke_data.columns)

# 5. Drop unnecessary columns
stroke_data.drop(columns=["Patient ID", "Patient Name"], inplace=True)

# 6. Visualize class distribution
sns.countplot(data=stroke_data, x="Diagnosis", palette={"Stroke": "green", "No Stroke": "red"})
plt.title("Distribution of Stroke Cases")
plt.show()

# 7. Train-test split (80/20)
X = stroke_data.drop("Diagnosis", axis=1)
y = stroke_data["Diagnosis"]

X = pd.get_dummies(X, drop_first=True)  # One-hot encode categorical variables, drop first column to avoid multicollinearity
y = y.cat.codes        # Convert labels to 0 and 1

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)

# 8. Balance training data using SMOTE + ENN (similar to ROSE)
from imblearn.combine import SMOTEENN
smote_enn = SMOTEENN(random_state=123)
X_train_bal, y_train_bal = smote_enn.fit_resample(X_train, y_train)

# 9. Standardize numeric features
scaler = StandardScaler()
X_train_bal = scaler.fit_transform(X_train_bal)
X_test = scaler.transform(X_test)

# 10. Train SVM model
svm_model = SVC(kernel='rbf', C=1, probability=True)
svm_model.fit(X_train_bal, y_train_bal)

# 11. Predict on test set
svm_predictions = svm_model.predict(X_test)

# 12. Evaluate SVM using confusion matrix
print("SVM Confusion Matrix:")
print(confusion_matrix(y_test, svm_predictions))
print("\nClassification Report:\n", classification_report(y_test, svm_predictions))

# 13. Visualize confusion matrix for SVM
ConfusionMatrixDisplay.from_predictions(y_test, svm_predictions, cmap="Blues")
plt.title("SVM Confusion Matrix")
plt.show()

# 14. ROC Curve for SVM
svm_probs = svm_model.predict_proba(X_test)[:, 1]
fpr_svm, tpr_svm, _ = roc_curve(y_test, svm_probs)
roc_auc_svm = auc(fpr_svm, tpr_svm)

plt.plot(fpr_svm, tpr_svm, color='blue', label=f'SVM (AUC = {roc_auc_svm:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title("SVM ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()

# 15. Logistic Regression Model
logit_model = LogisticRegression(max_iter=1000)
logit_model.fit(X_train_bal, y_train_bal)

# 16. Predict with Logistic Regression
logit_probs = logit_model.predict_proba(X_test)[:, 1]
logit_pred_classes = np.where(logit_probs > 0.5, 1, 0)

# 17. Confusion Matrix for Logistic Regression
print("Logistic Regression Confusion Matrix:")
print(confusion_matrix(y_test, logit_pred_classes))
print("\nClassification Report:\n", classification_report(y_test, logit_pred_classes))

# 18. ROC Curve for Logistic Regression
fpr_log, tpr_log, _ = roc_curve(y_test, logit_probs)
roc_auc_log = auc(fpr_log, tpr_log)

plt.plot(fpr_log, tpr_log, color='darkgreen', label=f'Logistic Regression (AUC = {roc_auc_log:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title("Logistic Regression ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()